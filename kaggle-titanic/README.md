# Kaggle Titanic

[Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic) is one of the most popular machine learning competitions on Kaggle. I made a Random Forest Classifier model and predicted survivability with an accuracy of 78.7%. This is in the 13th percentile of scores in this competition.

### Process
1. Data Cleaning & One-Hot Encoding
2. Model Optimization
3. Tuning Hyperparameters
4. Prediction

#### Libraries Used
- Pandas
- Numpy
- Scikit-learn
- Matplotlib

### Reflection
#### Pluses
I leveraged model architecture and parameters in random tree classifiers to avoid overfitting.

#### Deltas
If I were to do this challenge again, I would utilize feature engineering to make potentially relevant features such as extracting social class from names and determine deck from Cabin number. I could also spend more time performing exploratory analysis and excluding some irrelevant features from the start.

Look at my Kaggle profile [here](https://www.kaggle.com/serencha) and connect with me on [LinkedIn](https://www.linkedin.com/in/serenachang1/)!
