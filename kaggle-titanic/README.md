# Kaggle Titanic

[Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic) is one of the most popular machine learning competitions on Kaggle. I made a Random Forest Classifier model and predicted survivability with an accuracy of **78.7%**. This is in the 13th percentile of scores in this competition.

### Process
1. Data Cleaning & One-Hot Encoding
2. Model Optimization
3. Tuning Hyperparameters
4. Prediction

#### ðŸ“š Libraries Used
- Pandas
- Numpy
- Scikit-learn
- Matplotlib

### Reflection
#### ðŸ™Œ Pluses
I leveraged model architecture and parameters in random tree classifiers to avoid overfitting.

#### ðŸ’¡ Deltas
If I were to do this challenge again, I would utilize feature engineering to make potentially relevant features such as extracting social class from names and determine deck from Cabin number. I could also spend more time performing exploratory analysis and excluding some irrelevant features from the start.

I had much fun learning from this challenge. ðŸŽ‰ Look at my Kaggle profile [here](https://www.kaggle.com/serencha) and if you would like to connect, here is my [LinkedIn](https://www.linkedin.com/in/serenachang1/).
